{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color= \"#FA0246\">Sentiment Analysis</font></center>\n",
    "\n",
    "Sentiment Analysis is the process of determining whether a piece of writing is positive, negative or neutral. It’s also known as opinion mining, deriving the opinion or attitude of a speaker.A common use case for this technology is to discover how people feel about a particular topic.\n",
    "\n",
    "<img src=\"./Sentiment_Analysis_Image.jpg\" alt=\"Image of Sentiment Analysis\" style=\"height:600px; width:400\"/>\n",
    "\n",
    "Sentiment Analysis is a widlely adopted practise allover the world. In fact, sentiment analysis is useful in several areas whether in politics where we can analyse attitudes to goverment agencies or in Business analytics where we can track customers reviews. Sentiment Analysis is also used in psychology, sociology, law making...etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#B30233'>Project Context</font>\n",
    "This project is about people's reviews on social media concerning a famous Tunisian football player Youssef Msakni. \n",
    "Youssef Msakni (born 28 October 1990) is a Tunisian professional footballer who plays for Qatar Stars League club Al-Duhail.\n",
    "He was formed at Stade Tunisien, he evolves in July 2008 in the club of the ES Tunis. On November 17, he played his last game with ES Tunis but also his second final of the CAF Champions League against Al Ahly.\n",
    "\n",
    "<img src=\"./youssef_msakni.jpg\" alt=\"Youssef Mskani\" style=\"width:400px;height:600px\"/>\n",
    "<b>Career :<b>\n",
    "<ul>\n",
    " <li><strong>2017 : </strong>2018 FIFA World Cup qualification</li>\n",
    " <li><strong>2017 : </strong>2017 Africa Cup of Nations</li>\n",
    " <li><strong>2015 : </strong>2015 Africa Cup of Nations</li>\n",
    " <li><strong>2013: </strong>2013 Africa Cup of Nations</li>\n",
    "  <li><strong>2011 : </strong>Tunisian Cup with Esperance of Tunis.</li>\n",
    " <li><strong>2009, 2010, 2011, 2012 : </strong>Champion of Tunisia with Esperance of Tunis.</li>\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#B30233'>Data Description</font>\n",
    "For our project, we collected data about Youssef Msakni from his official Facebook page and from Youtube Videos using Youtube and Facebook APIs.\n",
    "This data is essentially the people's reviews of the posts, photos and videos concerning Youssef in the Tunisian Dialect dating back to year 2016, 2017 and 2018. \n",
    "We presented our data as a CSV file conntaing four columns : text_comment, sentiment_category, date, url where sentiment_category can be either positif or negatif.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Import entire libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#library for plotting stuff\n",
    "import matplotlib.pyplot as plt\n",
    "#so that plots appear in the same browser window\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "## Import specific items only from the sklearn library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#B30233'>Data preprocessing</font>\n",
    "In this section, we are going to import our raw data, to create a new binary sentiment category and to split the data into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data from a CSV file\n",
    "data = pd.read_csv('./youssef_msakni.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>sentiment_category</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sa77a nemes mnawer frr</td>\n",
       "      <td>positif</td>\n",
       "      <td>2017-10-11T01:14:34+0000</td>\n",
       "      <td>https://www.facebook.com/YoussefMsekni.fans/ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pff</td>\n",
       "      <td>negatif</td>\n",
       "      <td>2011-12-26T09:39:58+0000</td>\n",
       "      <td>https://www.facebook.com/Youssef.28.Msakni/pho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>le meilleur joueur de la tunisie cest msakni</td>\n",
       "      <td>positif</td>\n",
       "      <td>2017-05-18T18:33:41.000Z</td>\n",
       "      <td>https://www.youtube.com/watch?v=KV2WpPMM90E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yassir tfaded</td>\n",
       "      <td>negatif</td>\n",
       "      <td>2017-11-13T18:20:15+0000</td>\n",
       "      <td>https://www.facebook.com/YoussefMsekni.fans/ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M3allem</td>\n",
       "      <td>positif</td>\n",
       "      <td>2017-12-22T08:43:10+0000</td>\n",
       "      <td>https://www.facebook.com/Youssef.7.Msekni/phot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   comment_text sentiment_category  \\\n",
       "0                        Sa77a nemes mnawer frr            positif   \n",
       "1                                           pff            negatif   \n",
       "2  le meilleur joueur de la tunisie cest msakni            positif   \n",
       "3                                 Yassir tfaded            negatif   \n",
       "4                                       M3allem            positif   \n",
       "\n",
       "                       date                                                url  \n",
       "0  2017-10-11T01:14:34+0000  https://www.facebook.com/YoussefMsekni.fans/ph...  \n",
       "1  2011-12-26T09:39:58+0000  https://www.facebook.com/Youssef.28.Msakni/pho...  \n",
       "2  2017-05-18T18:33:41.000Z        https://www.youtube.com/watch?v=KV2WpPMM90E  \n",
       "3  2017-11-13T18:20:15+0000  https://www.facebook.com/YoussefMsekni.fans/ph...  \n",
       "4  2017-12-22T08:43:10+0000  https://www.facebook.com/Youssef.7.Msekni/phot...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaing the first 5 comments \n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new binary sentiment category variable\n",
    "We are creating here a new binary sentiment category where every positif comment has the value 1 and every negatif comment has the value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new binary sentiment_category variable:\n",
    "#   = 1 if rating == 'positif'\n",
    "#   = 0 if rating == 'negatif'\n",
    "z = np.where(data['sentiment_category'] =='positif', 1, 0)\n",
    "data['sentiment_category_Binary'] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment_category_Binary\n",
       "0     97\n",
       "1    312\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the size of each binary sentiment_category\n",
    "gR = data.groupby('sentiment_category_Binary').size()\n",
    "print(type(gR))\n",
    "gR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 2 artists>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM+glqXMGvSR1zqCXpM79P7XQ3ovV2BOdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca250aff60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(gR.index, gR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>sentiment_category</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>sentiment_category_Binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sa77a nemes mnawer frr</td>\n",
       "      <td>positif</td>\n",
       "      <td>2017-10-11T01:14:34+0000</td>\n",
       "      <td>https://www.facebook.com/YoussefMsekni.fans/ph...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>le meilleur joueur de la tunisie cest msakni</td>\n",
       "      <td>positif</td>\n",
       "      <td>2017-05-18T18:33:41.000Z</td>\n",
       "      <td>https://www.youtube.com/watch?v=KV2WpPMM90E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M3allem</td>\n",
       "      <td>positif</td>\n",
       "      <td>2017-12-22T08:43:10+0000</td>\n",
       "      <td>https://www.facebook.com/Youssef.7.Msekni/phot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aller Yuossef</td>\n",
       "      <td>positif</td>\n",
       "      <td>2017-07-05T02:20:51.000Z</td>\n",
       "      <td>https://www.youtube.com/watch?v=i2WUo3tzM6Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mnawer ya msakni</td>\n",
       "      <td>positif</td>\n",
       "      <td>2017-10-10T17:22:00+0000</td>\n",
       "      <td>https://www.facebook.com/YoussefMsekni.fans/ph...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   comment_text sentiment_category  \\\n",
       "0                        Sa77a nemes mnawer frr            positif   \n",
       "2  le meilleur joueur de la tunisie cest msakni            positif   \n",
       "4                                       M3allem            positif   \n",
       "5                                 aller Yuossef            positif   \n",
       "6                              Mnawer ya msakni            positif   \n",
       "\n",
       "                       date  \\\n",
       "0  2017-10-11T01:14:34+0000   \n",
       "2  2017-05-18T18:33:41.000Z   \n",
       "4  2017-12-22T08:43:10+0000   \n",
       "5  2017-07-05T02:20:51.000Z   \n",
       "6  2017-10-10T17:22:00+0000   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.facebook.com/YoussefMsekni.fans/ph...   \n",
       "2        https://www.youtube.com/watch?v=KV2WpPMM90E   \n",
       "4  https://www.facebook.com/Youssef.7.Msekni/phot...   \n",
       "5        https://www.youtube.com/watch?v=i2WUo3tzM6Y   \n",
       "6  https://www.facebook.com/YoussefMsekni.fans/ph...   \n",
       "\n",
       "   sentiment_category_Binary  \n",
       "0                          1  \n",
       "2                          1  \n",
       "4                          1  \n",
       "5                          1  \n",
       "6                          1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some positive examples\n",
    "data[data['sentiment_category_Binary'] == 1].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>sentiment_category</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>sentiment_category_Binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pff</td>\n",
       "      <td>negatif</td>\n",
       "      <td>2011-12-26T09:39:58+0000</td>\n",
       "      <td>https://www.facebook.com/Youssef.28.Msakni/pho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yassir tfaded</td>\n",
       "      <td>negatif</td>\n",
       "      <td>2017-11-13T18:20:15+0000</td>\n",
       "      <td>https://www.facebook.com/YoussefMsekni.fans/ph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Far5 cha9nous</td>\n",
       "      <td>negatif</td>\n",
       "      <td>2017-11-19T21:27:25+0000</td>\n",
       "      <td>https://www.facebook.com/Youssef.7.Msekni/phot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hetha el cha5ss min anwe3 el 7atharet el nadra</td>\n",
       "      <td>negatif</td>\n",
       "      <td>2017-09-15T14:05:55.000Z</td>\n",
       "      <td>https://www.youtube.com/watch?v=YXGdt6ppv5c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>moch rajel w 9oaalit el rojlla</td>\n",
       "      <td>negatif</td>\n",
       "      <td>2017-07-25T19:43:26.000Z</td>\n",
       "      <td>https://www.youtube.com/watch?v=YXGdt6ppv5c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      comment_text sentiment_category  \\\n",
       "1                                              pff            negatif   \n",
       "3                                    Yassir tfaded            negatif   \n",
       "11                                   Far5 cha9nous            negatif   \n",
       "14  hetha el cha5ss min anwe3 el 7atharet el nadra            negatif   \n",
       "18                  moch rajel w 9oaalit el rojlla            negatif   \n",
       "\n",
       "                        date  \\\n",
       "1   2011-12-26T09:39:58+0000   \n",
       "3   2017-11-13T18:20:15+0000   \n",
       "11  2017-11-19T21:27:25+0000   \n",
       "14  2017-09-15T14:05:55.000Z   \n",
       "18  2017-07-25T19:43:26.000Z   \n",
       "\n",
       "                                                  url  \\\n",
       "1   https://www.facebook.com/Youssef.28.Msakni/pho...   \n",
       "3   https://www.facebook.com/YoussefMsekni.fans/ph...   \n",
       "11  https://www.facebook.com/Youssef.7.Msekni/phot...   \n",
       "14        https://www.youtube.com/watch?v=YXGdt6ppv5c   \n",
       "18        https://www.youtube.com/watch?v=YXGdt6ppv5c   \n",
       "\n",
       "    sentiment_category_Binary  \n",
       "1                           0  \n",
       "3                           0  \n",
       "11                          0  \n",
       "14                          0  \n",
       "18                          0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some negative examples\n",
    "data[data['sentiment_category_Binary'] == 0].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306,)\n",
      "(103,)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data['comment_text'], \n",
    "                                                    data['sentiment_category_Binary'], \n",
    "                                                    random_state=591)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265    J'adorerais voir cette été msakni a l'om ou o ...\n",
       "279                                   hĥhhhhh ya youssif\n",
       "74                                      Rabi yhanik Nems\n",
       "96                Faraahna ya youssef fi l mondiale !! ❤\n",
       "322                               Sadkouni hkéytou férga\n",
       "346                       Bon année ya nems ya 4ali❤❤❤❤❤\n",
       "240                                               Mahlek\n",
       "89                                  best so proud of him\n",
       "53                                       Nîmes ya m3alem\n",
       "248                                     yezi mel tbourib\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some train examples\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#B30233'>Implementing our Sentiment Analyzer </font>\n",
    "After preprocessing our data, the first step to begin with in our Sentiment Analyzer is to tokenize our data and construct our vocabulary set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize documents & build vocabulary set\n",
    "This step could be done using the CountVectorizer class or the TfidVectorizer class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#A79533'>1- Using CountVectorizer with default parameters</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'>a/ Create CountVectorizer instance </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "# Learn the vocabulary (distinct words) of the input corpus\n",
    "vect.fit(X_train)\n",
    "print (len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'>b/Construct document-term matrix</font>\n",
    "In the second step, we extract features that represent raw text documents as numerical feature vectors.\n",
    "With the feature vectors and a given set of raw documents, we construct the document-term matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306, 766)\n",
      "(103, 766)\n"
     ]
    }
   ],
   "source": [
    "# The document-term matrix for the training corpus\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "print(X_train_vectorized.shape)\n",
    "# The document-term matrix for the testing corpus\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "print(X_test_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#A79533'>2- Using CountVectorizer with max_features=10 and  min_df=0.1 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'>a/ Create CountVectorizer instance </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766\n"
     ]
    }
   ],
   "source": [
    "vect1 = CountVectorizer(max_features=10, min_df=0.1)\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "# Learn the vocabulary (distinct words) of the input corpus\n",
    "vect1.fit(X_train)\n",
    "print (len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'>b/Construct document-term matrix</font>\n",
    "In the second step, we extract features that represent raw text documents as numerical feature vectors.\n",
    "With the feature vectors and a given set of raw documents, we construct the document-term matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306, 1)\n",
      "(103, 1)\n"
     ]
    }
   ],
   "source": [
    "# The document-term matrix for the training corpus\n",
    "X_train_vectorized_1 = vect1.transform(X_train)\n",
    "print(X_train_vectorized_1.shape)\n",
    "# The document-term matrix for the testing corpus\n",
    "X_test_vectorized_1 = vect1.transform(X_test)\n",
    "print(X_test_vectorized_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#A79533'>3- Using TfidfVectorizer</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'>a/ Create TfidfVectorizer instance </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create class instance\n",
    "tf_vect = TfidfVectorizer(use_idf = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'>b/ Tokenizing and constructing vocabulary</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize and construct vocabulary by calling fit() method\n",
    "tf_vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'>c/ Constructing tf-idf feature matrix for the training corpus</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#construct tf-idf feature matrix for the training corpus\n",
    "tf_vect.fit(X_train)\n",
    "train_tf_mat = tf_vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306, 766)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tf_mat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 765)\t0.350158081134\n",
      "  (0, 670)\t0.350158081134\n",
      "  (0, 538)\t0.309932172673\n",
      "  (0, 532)\t0.350158081134\n",
      "  (0, 469)\t0.213699031016\n",
      "  (0, 186)\t0.350158081134\n",
      "  (0, 161)\t0.350158081134\n",
      "  (0, 139)\t0.350158081134\n",
      "  (0, 70)\t0.350158081134\n",
      "  (1, 746)\t0.667675785597\n",
      "  (1, 704)\t0.329269024741\n",
      "  (1, 265)\t0.667675785597\n",
      "  (2, 731)\t0.742512675086\n",
      "  (2, 552)\t0.431694212785\n",
      "  (2, 502)\t0.51216699814\n",
      "  (3, 745)\t0.341284931489\n",
      "  (3, 704)\t0.263764573031\n",
      "  (3, 458)\t0.573380221827\n",
      "  (3, 224)\t0.39550358783\n",
      "  (3, 215)\t0.573380221827\n",
      "  (4, 581)\t0.57735026919\n",
      "  (4, 259)\t0.57735026919\n",
      "  (4, 239)\t0.57735026919\n",
      "  (5, 704)\t0.461414380475\n",
      "  (5, 502)\t0.345935469759\n",
      "  :\t:\n",
      "  (300, 321)\t0.200199386156\n",
      "  (300, 287)\t0.214621999841\n",
      "  (300, 278)\t0.214621999841\n",
      "  (300, 218)\t0.214621999841\n",
      "  (300, 108)\t0.214621999841\n",
      "  (300, 107)\t0.214621999841\n",
      "  (300, 0)\t0.214621999841\n",
      "  (301, 334)\t0.707106781187\n",
      "  (301, 65)\t0.707106781187\n",
      "  (302, 529)\t0.555565250983\n",
      "  (302, 356)\t0.527168044798\n",
      "  (302, 325)\t0.471928158701\n",
      "  (302, 274)\t0.436720640076\n",
      "  (303, 416)\t1.0\n",
      "  (304, 716)\t0.37551256251\n",
      "  (304, 552)\t0.218321660361\n",
      "  (304, 503)\t0.350278091547\n",
      "  (304, 496)\t0.350278091547\n",
      "  (304, 493)\t0.37551256251\n",
      "  (304, 350)\t0.37551256251\n",
      "  (304, 224)\t0.259019338467\n",
      "  (304, 204)\t0.37551256251\n",
      "  (304, 22)\t0.275347804914\n",
      "  (305, 452)\t0.610213673591\n",
      "  (305, 237)\t0.792236879073\n"
     ]
    }
   ],
   "source": [
    "print(train_tf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262                etfaker el mout hezha em3ak rak tansa\n",
      "9                                                Nemms❤❤\n",
      "352    Continue comme ça frère inchallah plus d'abonn...\n",
      "37                                 best player in africa\n",
      "376              il sont trop beau Machaallah 😘 💕 💖 💞💟💜💗\n",
      "288                                                 Nems\n",
      "405                                       Mnawer ya chi5\n",
      "174                                 Mhlkom nbo zinkom😘😘😘\n",
      "161                    i love you Yousef Msakni <3<3<3<3\n",
      "358                                      sayib rou7k 3ad\n",
      "257                                      yassir fasadtha\n",
      "320                                          rabi izidou\n",
      "131                                             ya tafeh\n",
      "193                     Ya3tek essa7a Yousef bon courage\n",
      "184                                       ♥♥ mséknyyy ♥♥\n",
      "91                                  We7dek ye m3elem😆😄👏😘\n",
      "271                                            ya baiou3\n",
      "323                                     Sa7a sa7a broo❤❤\n",
      "205                                    rabii ihani nchll\n",
      "244                                        bara ma5yebek\n",
      "301    amazing skills !!! outstanding player !!!\\nI'v...\n",
      "116                                         Sob7ane alah\n",
      "195               aaa amn ija m3a taraji frr rak m3allem\n",
      "249                      Rabbi yet9abbel mennek ya nemes\n",
      "80                 Tonf5ou fih belkedhbbbbb brabi na9sou\n",
      "67               moch rajel ya5i tefra7 ki martik t3arri\n",
      "112    allah yechfik w ydawik ye7lkik el ness el kol ...\n",
      "40                              afthal la3eb fi tounness\n",
      "144                          ma5yeb rassek ya  #msekniii\n",
      "11                                         Far5 cha9nous\n",
      "                             ...                        \n",
      "386                                Youssef ya 4ali 💋💋💋💋💋\n",
      "166                                     Mwaher ya msekni\n",
      "210                                        Mnawar yossef\n",
      "222                                      M3alem ya fanan\n",
      "148                                          mala youyou\n",
      "95                                         la3eb moomtez\n",
      "4                                                M3allem\n",
      "299                                 A7la Yosef nmot 3lik\n",
      "328                       mella 3bed w mella 9illit 7yéé\n",
      "68     futur ballon d'or africain ! msekni ! <3 ya m3...\n",
      "173       Slt yousef rabi yahmik salam à la najeh barcha\n",
      "373    Salam tu jou oumoi j'aimerai jouer un jour o q...\n",
      "99                                   rabi yhanik ye nims\n",
      "255    rabi yhanehom\\nama lcatégorie mte3 nes eli 7at...\n",
      "338                                        rabbi yhannik\n",
      "287                                  Yezzi bark  mrejtna\n",
      "289    Ya3tiiik saha w rabbi m3ak fara7t tournés lkol...\n",
      "147                                          A7la yousef\n",
      "217                                         rak 3al 7iit\n",
      "371                                   hĥhhhhh ya youssif\n",
      "146                             haha 6 bonto jabhom howa\n",
      "33                                 mela joueur ya msakni\n",
      "305                  Machahllah alikom beaugoss ya furet\n",
      "20                                        Ism el ronaya?\n",
      "215                     kol 3am wenti B 1000 5ir youssef\n",
      "160                                   Limsekni il kol ok\n",
      "404                                      mala youssef :)\n",
      "16                                             Ya m3alem\n",
      "24                El adhan y adhan ou houma dima yal3bou\n",
      "221                                          A7la ma3lme\n",
      "Name: comment_text, Length: 103, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test_tf_mat = tf_vect.transform(X_test)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches used in classification models\n",
    "\n",
    "Existing approaches to sentiment analysis can be grouped into three main categories: \n",
    "<ul>\n",
    "<li>Knowledge-based techniques</li>\n",
    "<li>Statistical methods</li>\n",
    "<li>Hybrid approaches</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Knowledge-based techniques</strong> classify text by affecting categories based on the presence of unambiguous affect words such as happy, sad, afraid, and bored.\n",
    "Some knowledge bases are not only a list containing obvious affect words, but also arbitrary words with a probable \"affinity\" to particular emotions.\n",
    "\n",
    "<strong>Statistical methods</strong> leverage on elements from machine learning such as latent semantic analysis, support vector machines and \"bag of words\". More sophisticated methods try to detect the holder of a sentiment (i.e., the person who maintains that affective state) and the target (i.e., the entity about which the affect is felt). To mine the opinion in context and get the feature about which the speaker has opined, the grammatical relationships of words are used. Grammatical dependency relations are obtained by deep parsing of the text.\n",
    "\n",
    "<strong>Hybrid approaches</strong> leverage on both machine learning and elements from knowledge representation such as ontologies and semantic networks in order to detect semantics that are expressed in a subtle manner.\n",
    "\n",
    "In our case we have choose <strong>Statistical methods</strong> such as <strong>Logistic Regression</strong> and <strong>Naive Bayes</strong>.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#7B0A29'>Build classification model using Logistic Regression</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#A79533'>1/ Using the feature vectors of the training documents deducted from countVectorizer with default parameters </font>\n",
    "We are going to to build a classification model using the feature vectors of the training documents (which are stored in the variable X_train_vectorized) and their corresponding true sentiment categories (which are stored in the variable Y_train)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'> a/ Training the model using Logistic Regression method </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using Logistic Regression method\n",
    "LR_model = LogisticRegression()\n",
    "LR_model.fit(X_train_vectorized, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'> b/ Test the classification model</font>\n",
    "We'll use the obtained Logistic Regression model to predict sentiment categories (classes) of test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(103,)\n"
     ]
    }
   ],
   "source": [
    "# Use this model to predict the sentiment category of test documents\n",
    "LR_predictions = LR_model.predict(X_test_vectorized)\n",
    "print(type(LR_predictions))\n",
    "print(LR_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 1]\n",
      "[0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(LR_predictions[:30])\n",
    "print(np.array(Y_test[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'> c/ Evaluating performance of classification model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84466019417475724"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_classif_rate = accuracy_score(Y_test, LR_predictions)\n",
    "LR_classif_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'> d/ Interpretation of model's coefficients (parameters)</font>\n",
    "Which vocabulary words are most important in our classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First get LR model's coefficient (there is one coefficient per vocabulary word)\n",
    "coefs = LR_model.coef_[0]\n",
    "\n",
    "# Sort these coefficient values in ascending order\n",
    "sorted_coef_index = coefs.argsort()  # sort by actual value\n",
    "sorted_coef_index_2 = abs(coefs).argsort()  # sort by absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs of LR model:\n",
      "\n",
      " Coefficient values:\n",
      "[-1.26411619 -1.24989107 -1.03605374 -0.91962515 -0.85581248 -0.84590117\n",
      " -0.81275124 -0.78384898 -0.77890546 -0.77890546]\n",
      "\n",
      " Feature names:\n",
      "['rajel' 'malla' 'tfouuuhhh' 'jabri' 'mel' 'yezi' 'el' 'mrith' 'tfaded'\n",
      " 'yassir']\n",
      "\n",
      "Largest Coefs of LR model:\n",
      "\n",
      " Coefficient values: \n",
      "[ 1.20263284  1.13533373  0.95875918  0.82230023  0.70018282  0.69282467\n",
      "  0.65914228  0.65006848  0.62453019  0.58186264]\n",
      " Feature names: \n",
      "['rabi' 'm3alem' 'nems' 'bravo' 'm3allem' 'msekni' 'mnawer' 'the' 'youssef'\n",
      " 'saha']\n",
      "Smallest abs(Coefs):\n",
      "\n",
      " Coefficient values:\n",
      "[ 0.0033895   0.0033895   0.0033895   0.01053676  0.01053676  0.01053676\n",
      "  0.01053676  0.01053676  0.01053676  0.01053676]\n",
      "\n",
      " Feature names:\n",
      "['europa' 'africa' 'america' 'wled' '3endha' 'kifkom' 'tefta5er'\n",
      " 'nousourna' 'rfe3toulna' 'rousna']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the 10 smallest and 10 largest coefficients\n",
    "\n",
    "# Feature_names = vect.get_feature_names()\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "print('Smallest Coefs of LR model:\\n')\n",
    "print(' Coefficient values:\\n{}\\n'.format(coefs[sorted_coef_index[:10]]))\n",
    "print(' Feature names:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "\n",
    "print('Largest Coefs of LR model:\\n')\n",
    "print(' Coefficient values: \\n{}'.format(coefs[sorted_coef_index[:-11:-1]]))\n",
    "print(' Feature names: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n",
    "\n",
    "print('Smallest abs(Coefs):\\n')\n",
    "print(' Coefficient values:\\n{}\\n'.format(coefs[sorted_coef_index_2[:10]]))\n",
    "print(' Feature names:\\n{}\\n'.format(feature_names[sorted_coef_index_2[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#A79533'>2/ Using the feature vectors of the training documents deducted from countVectorizer with max_features=10 and min_df=0.1 </font>\n",
    "We are going to to build a classification model using the feature vectors of the training documents (which are stored in the variable X_train_vectorized_1) and their corresponding true sentiment categories (which are stored in the variable Y_train)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'> a/ Training the model using Logistic Regression method </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using Logistic Regression method\n",
    "LR_model_1 = LogisticRegression()\n",
    "LR_model_1.fit(X_train_vectorized_1, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'> Testing the classification model</font>\n",
    "We'll use the obtained Logistic Regression model to predict sentiment categories (classes) of test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(103,)\n"
     ]
    }
   ],
   "source": [
    "# Use this model to predict the sentiment category of test documents\n",
    "LR_predictions_1 = LR_model_1.predict(X_test_vectorized_1)\n",
    "print(type(LR_predictions_1))\n",
    "print(LR_predictions_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(LR_predictions_1[:30])\n",
    "print(np.array(Y_test[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'>b/ Evaluating performance of classification model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76699029126213591"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_classif_rate_1 = accuracy_score(Y_test, LR_predictions_1)\n",
    "LR_classif_rate_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='#H69533'>c/ Interpretation of model's coefficients (parameters)</font>\n",
    "Which vocabulary words are most important in our classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First get LR model's coefficient (there is one coefficient per vocabulary word)\n",
    "coefs_1 = LR_model_1.coef_[0]\n",
    "\n",
    "# Sort these coefficient values in ascending order\n",
    "sorted_coef_index_1 = coefs_1.argsort()  # sort by actual value\n",
    "sorted_coef_index_3 = abs(coefs_1).argsort()  # sort by absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs of LR model:\n",
      "\n",
      " Coefficient values:\n",
      "[ 0.23582509]\n",
      "\n",
      " Feature names:\n",
      "['ya']\n",
      "\n",
      "Largest Coefs of LR model:\n",
      "\n",
      " Coefficient values: \n",
      "[ 0.23582509]\n",
      " Feature names: \n",
      "['ya']\n",
      "Smallest abs(Coefs):\n",
      "\n",
      " Coefficient values:\n",
      "[ 0.23582509]\n",
      "\n",
      " Feature names:\n",
      "['ya']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the 10 smallest and 10 largest coefficients\n",
    "\n",
    "# Feature_names = vect.get_feature_names()\n",
    "feature_names_1 = np.array(vect1.get_feature_names())\n",
    "\n",
    "print('Smallest Coefs of LR model:\\n')\n",
    "print(' Coefficient values:\\n{}\\n'.format(coefs_1[sorted_coef_index_1[:10]]))\n",
    "print(' Feature names:\\n{}\\n'.format(feature_names_1[sorted_coef_index_1[:10]]))\n",
    "\n",
    "print('Largest Coefs of LR model:\\n')\n",
    "print(' Coefficient values: \\n{}'.format(coefs_1[sorted_coef_index_1[:-11:-1]]))\n",
    "print(' Feature names: \\n{}'.format(feature_names_1[sorted_coef_index_1[:-11:-1]]))\n",
    "\n",
    "print('Smallest abs(Coefs):\\n')\n",
    "print(' Coefficient values:\\n{}\\n'.format(coefs_1[sorted_coef_index_3[:10]]))\n",
    "print(' Feature names:\\n{}\\n'.format(feature_names_1[sorted_coef_index_3[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = '#7B0A29'> Build classification model using Naive Bayes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build Naive Bayes classification model\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train_vectorized, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Use this model to predict sentiment of test documents\n",
    "\n",
    "NB_predictions = NB_model.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance of classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87378640776699024"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## calculate model's classification rate on the test corpus\n",
    "\n",
    "NB_classif_rate = accuracy_score(Y_test, NB_predictions)\n",
    "NB_classif_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <font color='#B30233'>Evaluation</font>\n",
    "The accuracy of a sentiment analysis system is mainly how well it agrees with human judgments. This is usually measured by variant measures based on precision and recall over the two target categories of negative and positive texts.\n",
    "\n",
    "Thus, a program which achieves <strong>70%</strong> accuracy in classifying sentiment is doing nearly as well as humans, even though such accuracy may not sound impressive. If a program were \"right\" <strong>100%</strong> of the time, humans would still disagree with it about <strong>20%</strong> of the time, since they disagree that much about any answer.\n",
    "\n",
    "In our case the<strong> accuracy</strong > is :\n",
    "<ul>\n",
    "<li><strong>0.84466019417475724</strong> for our Classification Model build with <strong>Linear Regression</strong></li>\n",
    "<li><strong>0.87378640776699024</strong> for our Classification Model build with<strong> Naive Bayes</strong></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
